{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mslikker/mslikker.github.io/blob/main/DISproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set-Up**"
      ],
      "metadata": {
        "id": "TVJc3ThbekpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install findspark\n",
        "!pip install -U -q PyDrive\n",
        "!pip install pyngrok\n",
        "!pip install graphframes\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "!wget https://repos.spark-packages.org/graphframes/graphframes/0.8.4-spark3.5-s_2.12/graphframes-0.8.4-spark3.5-s_2.12.jar\n",
        "!pip install graphframes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcCr5T9lenA2",
        "outputId": "f9c8e311-ad81-40e7-af2d-407fdb28035c",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: graphframes in /usr/local/lib/python3.10/dist-packages (0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from graphframes) (1.26.4)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.10/dist-packages (from graphframes) (1.3.7)\n",
            "openjdk-8-jdk-headless is already the newest version (8u422-b05-1~22.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "--2024-10-15 14:21:21--  https://repos.spark-packages.org/graphframes/graphframes/0.8.4-spark3.5-s_2.12/graphframes-0.8.4-spark3.5-s_2.12.jar\n",
            "Resolving repos.spark-packages.org (repos.spark-packages.org)... 13.35.166.111, 13.35.166.93, 13.35.166.66, ...\n",
            "Connecting to repos.spark-packages.org (repos.spark-packages.org)|13.35.166.111|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 247575 (242K) [binary/octet-stream]\n",
            "Saving to: â€˜graphframes-0.8.4-spark3.5-s_2.12.jar.1â€™\n",
            "\n",
            "graphframes-0.8.4-s 100%[===================>] 241.77K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-10-15 14:21:21 (18.0 MB/s) - â€˜graphframes-0.8.4-spark3.5-s_2.12.jar.1â€™ saved [247575/247575]\n",
            "\n",
            "Requirement already satisfied: graphframes in /usr/local/lib/python3.10/dist-packages (0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from graphframes) (1.26.4)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.10/dist-packages (from graphframes) (1.3.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now we authenticate a Google Drive client to download the file we will be processing in our Spark job.**"
      ],
      "metadata": {
        "id": "VcIP0cTze3vJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate and create the PyDrive client (ipython-input-2-6083209ed024)\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "lYO4Nk48e0ZJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9047e30b-f96e-4b40-b727-7dfde668694d"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:pydrive is deprecated and no longer maintained. We recommend that you migrate your projects to pydrive2, the maintained fork of pydrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Replace the ID with your own ID: put the file in your Google Drive and click 'copy link', the ID is in the link**"
      ],
      "metadata": {
        "id": "g0aOrJp7rtXE"
      }
    },
    {
      "source": [
        "id='1Pli2SAi1wEzY3eRsQNK1H3Erdko0_4di'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('telecom_calls_community.csv')"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "TougAG-rkz3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "metadata": {
        "id": "6U5de5xDfsqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install GraphFrames Python package\n",
        "!pip install graphframes\n",
        "\n",
        "# Download the GraphFrames JAR file\n",
        "!wget https://repos.spark-packages.org/graphframes/graphframes/0.8.4-spark3.5-s_2.12/graphframes-0.8.4-spark3.5-s_2.12.jar\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "\n",
        "# Path to the downloaded GraphFrames JAR file\n",
        "graphframes_jar_path = \"/content/graphframes-0.8.4-spark3.5-s_2.12.jar\"\n",
        "\n",
        "# Modify your existing Spark session configuration to include the GraphFrames JAR\n",
        "conf = SparkConf()\n",
        "conf.set(\"spark.ui.port\", \"4050\")\n",
        "conf.setAppName(\"project\")\n",
        "conf.setMaster(\"local[*]\")\n",
        "conf.set(\"spark.driver.memory\", \"2G\")\n",
        "conf.set(\"spark.driver.maxResultSize\", \"2g\")\n",
        "conf.set(\"spark.executor.memory\", \"1G\")\n",
        "conf.set(\"spark.jars\", graphframes_jar_path)  # Add GraphFrames JAR here\n",
        "\n",
        "# Create the Spark session\n",
        "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
        "\n",
        "spark\n"
      ],
      "metadata": {
        "id": "VHTFUldPfvgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Check the active sessions***"
      ],
      "metadata": {
        "id": "LuO4kpCxqrPm"
      }
    },
    {
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession # Import SparkSession explicitly\n",
        "\n",
        "# Check for active SparkSession\n",
        "if SparkSession.getActiveSession() is not None:  # Use getActiveSession() directly\n",
        "    print(\"Active SparkSession exists\")\n",
        "    print(SparkSession.getActiveSession())  # Get the active session\n",
        "else:\n",
        "    print(\"No active SparkSession\")\n",
        "\n",
        "# Check for active SparkContext\n",
        "if pyspark.SparkContext._active_spark_context is not None:\n",
        "    print(\"Active SparkContext exists\")\n",
        "    print(pyspark.SparkContext.getOrCreate())  # Get the active context\n",
        "else:\n",
        "    print(\"No active SparkContext\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "JCkUYtdAqbI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "id": "ULddQQEGf1fR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read the file\n",
        "telecom_calls_community = spark.read.csv(\"telecom_calls_community.csv\", header=True, inferSchema=True)\n",
        "# show a formatted version of the file\n",
        "telecom_calls_community.show(10)"
      ],
      "metadata": {
        "id": "nM6qQMvJf94q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# random things\n",
        "print(\"In total there are {0} calls\".format(telecom_calls_community.count()))"
      ],
      "metadata": {
        "id": "msmuslODlkId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Combine both caller columns\n",
        "all_callers = telecom_calls_community.select(\"caller_id_1\").union(telecom_calls_community.select(\"caller_id_2\"))\n",
        "\n",
        "# Count distinct callers\n",
        "num_different_callers = all_callers.distinct().count()\n",
        "\n",
        "print(f\"Number of different callers: {num_different_callers}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Iu0H-4Y1l8P0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding communities using Connected Components"
      ],
      "metadata": {
        "id": "XZ5LS8UNo6bs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df = spark.read.csv(\"telecom_calls_community.csv\", header=True, inferSchema=True)\n",
        "df.show(5)\n",
        "\n",
        "# Rename columns to src which stands for 'source_node' and dst which stands for'destination_node'\n",
        "edges = df.selectExpr(\"caller_id_1 as src\", \"caller_id_2 as dst\")\n",
        "edges.show(20)\n",
        "\n",
        "# Convert edges DataFrame to an RDD of (source, destination) pairs\n",
        "edges_rdd = edges.rdd.map(lambda row: (row['src'], row['dst']))\n",
        "\n",
        "print(\"RDD data:\")\n",
        "print(edges_rdd.take(5))\n",
        "\n",
        "\n",
        "# for every source, if the id is the same in a row (for example in row 2: src is 27 and in row 17: src is 27, but the dst is 14 in row 2 and 8 in row 17), i want it to"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4X6wbiq2s1Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using GraphFrames to identify communities in large datasets"
      ],
      "metadata": {
        "id": "t_hjGWx-2Gn4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset**"
      ],
      "metadata": {
        "id": "u5Ju1ikbxdm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from graphframes import GraphFrame # This imports GraphFrame from the graphframes library\n",
        "\n",
        "# Create vertices DataFrame\n",
        "vertices = telecom_calls_community.select(\"caller_id_1\").distinct().withColumnRenamed(\"caller_id_1\", \"id\").union(telecom_calls_community.select(\"caller_id_2\").distinct().withColumnRenamed(\"caller_id_2\", \"id\")).distinct()\n",
        "\n",
        "# Create the GraphFrame\n",
        "g = GraphFrame(vertices, edges) # We can use GraphFrame class now since we imported it.\n",
        "\n",
        "# Show vertices and edges\n",
        "g.vertices.show()\n",
        "g.edges.show()\n",
        "\n",
        "# Set checkpoint directory before running connected components\n",
        "spark.sparkContext.setCheckpointDir(\"/tmp/checkpoint\") # Choose a suitable directory\n",
        "\n",
        "# Run the connected components algorithm\n",
        "result = g.connectedComponents()\n",
        "result.show()\n"
      ],
      "metadata": {
        "id": "5yIU2fP_xfaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WORK IN PROGRESS minhash"
      ],
      "metadata": {
        "id": "ZBKC11cVRRE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TRY OUT\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, DoubleType\n",
        "from pyspark.ml.linalg import Vectors\n",
        "import numpy as np\n",
        "import hashlib\n",
        "\n",
        "\n",
        "#Transform into DF where each row represents a community and its nodes as a set\n",
        "community_nodes = result.groupBy(\"component\").agg(F.collect_list(\"id\").alias(\"nodes\"))\n",
        "\n",
        "# Show the community nodes DataFrame\n",
        "community_nodes.show()"
      ],
      "metadata": {
        "id": "Li-ECLECRQoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries (partially redundant)\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, DoubleType\n",
        "from pyspark.ml.linalg import Vectors\n",
        "import numpy as np\n",
        "import hashlib\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "# Transform into DF where each row represents a community and its nodes as a set\n",
        "community_nodes = result.groupBy(\"component\").agg(F.collect_list(\"id\").alias(\"nodes\"))\n",
        "\n",
        "# Show the community nodes DataFrame\n",
        "community_nodes.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "AVD2wsZFS0ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of hash functions (we can set what we think right)\n",
        "num_hashes = 30\n",
        "\n",
        "# Hashing function to produce MinHash signatures\n",
        "def min_hash_signature(nodes):\n",
        "    # Create a random hash function\n",
        "    random_hashes = np.random.randint(1, 1000, size=num_hashes)  # Random hash seeds\n",
        "    min_hash_values = []\n",
        "    for i in range(num_hashes):\n",
        "        # Apply the hash function to each node and take the minimum\n",
        "        hashed_values = [hash(node) % random_hashes[i] for node in nodes]\n",
        "        min_hash_values.append(min(hashed_values))\n",
        "    return Vectors.dense(min_hash_values)"
      ],
      "metadata": {
        "id": "YKugnZ5zTEFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Register user defined function\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, DoubleType\n",
        "\n",
        "min_hash_udf = udf(lambda nodes: min_hash_signature(nodes).toArray().tolist(), ArrayType(DoubleType()))\n",
        "\n",
        "# Apply the MinHashing to each community\n",
        "community_signatures = community_nodes.withColumn(\"signature\", min_hash_udf(\"nodes\"))\n",
        "\n",
        "print(community_signatures)\n",
        "\n",
        "# Show the community signatures DataFrame\n",
        "#community_signatures.show(truncate=False)"
      ],
      "metadata": {
        "id": "SzdFaNLaTVvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WORK IN PROGRESS END"
      ],
      "metadata": {
        "id": "fo5w_ve8SoBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Louvain Algorithm (via Label Propagation)"
      ],
      "metadata": {
        "id": "c_YajUftyz9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you have to set the max iterations"
      ],
      "metadata": {
        "id": "HE7UoouC1UTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from graphframes import GraphFrame\n",
        "\n",
        "def run_louvain_algorithm(g):\n",
        "    print(\"Running Louvain (Label Propagation) algorithm...\")\n",
        "    start_time = time.time()\n",
        "    louvain_result = g.labelPropagation(maxIter=10)  # Set max iterations\n",
        "    execution_time = time.time() - start_time\n",
        "    print(f\"Louvain (Label Propagation) execution time: {execution_time:.2f} seconds\")\n",
        "    louvain_result.show(10)\n",
        "    return louvain_result\n",
        "\n",
        "vertices = telecom_calls_community.select(\"caller_id_1\").distinct().withColumnRenamed(\"caller_id_1\", \"id\").union(\n",
        "    telecom_calls_community.select(\"caller_id_2\").distinct().withColumnRenamed(\"caller_id_2\", \"id\")).distinct()\n",
        "\n",
        "edges = telecom_calls_community.selectExpr(\"caller_id_1 as src\", \"caller_id_2 as dst\")\n",
        "\n",
        "g = GraphFrame(vertices, edges)\n",
        "\n",
        "spark.sparkContext.setCheckpointDir(\"/tmp/checkpoint\")\n",
        "\n",
        "louvain_result = run_louvain_algorithm(g)\n",
        "\n"
      ],
      "metadata": {
        "id": "W7aJlYaby4S1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Cliques Algorithm"
      ],
      "metadata": {
        "id": "SHptjl40zXUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I think this is not relevant for us, you have to define K but we dont want that"
      ],
      "metadata": {
        "id": "9abQIyOI1JVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from graphframes import GraphFrame\n",
        "import time\n",
        "\n",
        "def find_k_cliques(g, k):\n",
        "    \"\"\"\n",
        "    Find K-Cliques in the graph using a custom PySpark implementation.\n",
        "    \"\"\"\n",
        "    print(f\"Running K-Cliques algorithm for k={k}...\")\n",
        "\n",
        "    # Measure start time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Initialize the cliques as 2-cliques (edges of the graph)\n",
        "    cliques = g.edges.select(F.col(\"src\").alias(\"node1\"), F.col(\"dst\").alias(\"node2\"))\n",
        "\n",
        "    # Iteratively join to find k-cliques\n",
        "    for i in range(2, k):\n",
        "        cliques = cliques.join(g.edges, cliques[\"node\" + str(i)] == g.edges[\"src\"]) \\\n",
        "                         .select([F.col(\"node\" + str(j)) for j in range(1, i+1)] + [F.col(\"dst\").alias(\"node\" + str(i+1))]) \\\n",
        "                         .distinct()\n",
        "\n",
        "    # Display the found k-cliques\n",
        "    print(f\"Found {cliques.count()} {k}-cliques\")\n",
        "    cliques.show(10)\n",
        "\n",
        "    # Measure execution time\n",
        "    execution_time = time.time() - start_time\n",
        "    print(f\"K-Cliques execution time: {execution_time:.2f} seconds\")\n",
        "\n",
        "    return cliques\n",
        "\n",
        "# Prepare vertices and edges as before\n",
        "vertices = telecom_calls_community.select(\"caller_id_1\").distinct().withColumnRenamed(\"caller_id_1\", \"id\").union(\n",
        "    telecom_calls_community.select(\"caller_id_2\").distinct().withColumnRenamed(\"caller_id_2\", \"id\")).distinct()\n",
        "\n",
        "edges = telecom_calls_community.selectExpr(\"caller_id_1 as src\", \"caller_id_2 as dst\")\n",
        "\n",
        "# Create the GraphFrame\n",
        "g = GraphFrame(vertices, edges)\n",
        "\n",
        "# Run the K-Cliques algorithm with k=3 (for example)\n",
        "k = 3\n",
        "k_cliques_result = find_k_cliques(g, k)\n"
      ],
      "metadata": {
        "id": "wYBBq8BXzEVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connected Components"
      ],
      "metadata": {
        "id": "Izkb5fph0MqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from graphframes import GraphFrame\n",
        "\n",
        "def run_connected_components(g):\n",
        "    print(\"Running Connected Components algorithm...\")\n",
        "    start_time = time.time()\n",
        "    result = g.connectedComponents()\n",
        "    execution_time = time.time() - start_time\n",
        "    print(f\"Connected Components execution time: {execution_time:.2f} seconds\")\n",
        "    result.show(10)\n",
        "    return result\n",
        "\n",
        "vertices = telecom_calls_community.select(\"caller_id_1\").distinct().withColumnRenamed(\"caller_id_1\", \"id\").union(\n",
        "    telecom_calls_community.select(\"caller_id_2\").distinct().withColumnRenamed(\"caller_id_2\", \"id\")).distinct()\n",
        "\n",
        "edges = telecom_calls_community.selectExpr(\"caller_id_1 as src\", \"caller_id_2 as dst\")\n",
        "\n",
        "g = GraphFrame(vertices, edges)\n",
        "\n",
        "spark.sparkContext.setCheckpointDir(\"/tmp/checkpoint\")\n",
        "\n",
        "connected_components_result = run_connected_components(g)\n",
        "\n",
        "# Save the output as CSV for Cytoscape visualisation\n",
        "# 1. Save nodes (id, component) - the result of the connected components algorithm\n",
        "nodes_df = connected_components_result.toPandas()\n",
        "nodes_df.to_csv(\"nodes.csv\", index=False)\n",
        "print(\"Nodes data saved to 'nodes.csv'.\")\n",
        "\n",
        "# 2. Save edges (src, dst) - the original edges connecting the nodes\n",
        "edges_df = edges.toPandas()\n",
        "edges_df.to_csv(\"edges.csv\", index=False)\n",
        "print(\"Edges data saved to 'edges.csv'.\")\n"
      ],
      "metadata": {
        "id": "EeJpXa5Z0MPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Further analysis on Connected components results:**"
      ],
      "metadata": {
        "id": "Q-Bt9xkO18uZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_components = connected_components_result.select(\"component\").distinct().count()\n",
        "print(f\"Total number of connected components: {num_components}\")\n",
        "\n",
        "component_sizes = connected_components_result.groupBy(\"component\").count().orderBy(\"count\", ascending=False)\n",
        "component_sizes.show(10)  # Show the sizes of the top 10 largest components\n",
        "print(\"Component = community; count = amount of connected nodes\")\n"
      ],
      "metadata": {
        "id": "vv_NwEx-2CYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional Libraries\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "from fastdtw import fastdtw\n",
        "from scipy.spatial.distance import euclidean"
      ],
      "metadata": {
        "id": "cVDFVU86_6bV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "def convert_to_networkx(connected_components_result, edges):\n",
        "    \"\"\"\n",
        "    Convert the connected components to NetworkX graphs for each community.\n",
        "    \"\"\"\n",
        "    community_graphs = []\n",
        "    component_ids = connected_components_result.select(\"component\").distinct().collect()\n",
        "\n",
        "    for component in component_ids:\n",
        "        component_id = component['component']\n",
        "        component_data = connected_components_result.filter(col(\"component\") == component_id)\n",
        "\n",
        "        G = nx.Graph()\n",
        "\n",
        "        # Collect node IDs as a set for better filtering in PySpark\n",
        "        node_ids = {row['id'] for row in component_data.collect()}\n",
        "\n",
        "        # Add nodes to the graph\n",
        "        for node_id in node_ids:\n",
        "            G.add_node(node_id)\n",
        "\n",
        "        # Filter edges that are part of the current component\n",
        "        edges_in_component = edges.filter(\n",
        "            (edges['src'].isin(node_ids)) |\n",
        "            (edges['dst'].isin(node_ids))\n",
        "        ).select(col(\"src\"), col(\"dst\"))\n",
        "\n",
        "        # Add the edges to the graph\n",
        "        for edge in edges_in_component.collect():\n",
        "            G.add_edge(edge['src'], edge['dst'])\n",
        "\n",
        "        community_graphs.append(G)\n",
        "\n",
        "    return community_graphs\n"
      ],
      "metadata": {
        "id": "cwRWZMyc_6df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert connected components to NetworkX graphs\n",
        "community_graphs = convert_to_networkx(connected_components_result, edges)"
      ],
      "metadata": {
        "id": "ZSg1rYl__6gK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "def visualize_community_graphs(community_graphs, num_to_visualize=3):\n",
        "    \"\"\"\n",
        "    Function to visualize a few community graphs and print their nodes and edges.\n",
        "    Args:\n",
        "        community_graphs (list): A list of NetworkX graphs representing each community.\n",
        "        num_to_visualize (int): The number of community graphs to visualize.\n",
        "    \"\"\"\n",
        "    for i, G in enumerate(community_graphs[:num_to_visualize]):\n",
        "        print(f\"Community {i + 1}:\")\n",
        "        print(f\"Nodes: {G.nodes()}\")\n",
        "        print(f\"Edges: {G.edges()}\")\n",
        "\n",
        "        # Draw the graph\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        nx.draw(G, with_labels=True, node_color=\"skyblue\", node_size=500, edge_color=\"gray\")\n",
        "        plt.title(f\"Community {i + 1}\")\n",
        "        plt.show()\n",
        "\n",
        "# Example usage: Visualize and inspect the first 3 community graphs\n",
        "visualize_community_graphs(community_graphs, num_to_visualize=6)"
      ],
      "metadata": {
        "id": "cfwi32Hy_6jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create graphlet vectors\n",
        "def get_graphlet_vector(G):\n",
        "    \"\"\"\n",
        "    Create a vector representing the graphlets (subgraphs) in G.\n",
        "    For simplicity, this will be a vector of node degrees.\n",
        "    \"\"\"\n",
        "    nodes = list(G.nodes)\n",
        "    vector = []\n",
        "    for node in nodes:\n",
        "        degree = G.degree[node]  # Get the degree (number of connections) of the node\n",
        "        vector.append(degree)\n",
        "    return np.array(vector)"
      ],
      "metadata": {
        "id": "_PbYj7O5CIdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from builtins import max as builtin_max  # Import the built-in max function\n",
        "\n",
        "# 3. Function to Calculate gcd_11 (Spatial Similarity)\n",
        "def calculate_gcd_11(communities):\n",
        "    \"\"\"\n",
        "    Function to calculate GCD-11 distance between pairs of communities.\n",
        "    \"\"\"\n",
        "    # Find the maximum size of any graphlet vector\n",
        "    max_vector_length = builtin_max([len(get_graphlet_vector(g)) for g in communities]) # Use the built-in max\n",
        "\n",
        "    gcd_11_distances = {}\n",
        "\n",
        "    for i, j in combinations(range(len(communities)), 2):\n",
        "        g1 = communities[i]\n",
        "        g2 = communities[j]\n",
        "\n",
        "        v1 = get_graphlet_vector(g1)\n",
        "        v2 = get_graphlet_vector(g2)\n",
        "\n",
        "        # Pad the shorter vector with zeros to match the length of the longer one\n",
        "        if len(v1) < max_vector_length:\n",
        "            v1 = np.pad(v1, (0, max_vector_length - len(v1)))\n",
        "        if len(v2) < max_vector_length:\n",
        "            v2 = np.pad(v2, (0, max_vector_length - len(v2)))\n",
        "\n",
        "        # Calculate the GCD-11 distance using Euclidean distance\n",
        "        distance = euclidean(v1, v2)\n",
        "        gcd_11_distances[(i, j)] = distance\n",
        "\n",
        "    return gcd_11_distances"
      ],
      "metadata": {
        "id": "C3ZxAWqEKlcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Extract Time Sequences (for DTW)\n",
        "def extract_time_sequences(connected_components_result, df):\n",
        "    \"\"\"\n",
        "    Extract the start and end times of interactions in each community.\n",
        "    \"\"\"\n",
        "    community_time_sequences = []\n",
        "    component_ids = connected_components_result.select(\"component\").distinct().collect()\n",
        "\n",
        "    for component in component_ids:\n",
        "        component_id = component['component']\n",
        "        component_data = connected_components_result.filter(col(\"component\") == component_id)\n",
        "\n",
        "        # Correct the component_data filtering\n",
        "        time_sequence = df.filter(\n",
        "            (df['caller_id_1'].isin([row['id'] for row in component_data.collect()])) |\n",
        "            (df['caller_id_2'].isin([row['id'] for row in component_data.collect()]))\n",
        "        ).select(col(\"start_time\"), col(\"end_time\")).collect()\n",
        "\n",
        "        community_time_sequences.append(time_sequence)\n",
        "\n",
        "    return community_time_sequences"
      ],
      "metadata": {
        "id": "F5wgcl5CCIfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract time sequences for communities\n",
        "community_time_sequences = extract_time_sequences(connected_components_result, telecom_calls_community)"
      ],
      "metadata": {
        "id": "0RxqMYKfCIiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Function to Calculate DTW (Temporal Similarity)\n",
        "def calculate_dtw(time_sequences):\n",
        "    \"\"\"\n",
        "    Function to calculate DTW (Dynamic Time Warping) distance between pairs of time sequences.\n",
        "\n",
        "    Args:\n",
        "        time_sequences (list): A list of time sequences (start and end times for each community).\n",
        "\n",
        "    Returns:\n",
        "        dtw_distances (dict): A dictionary where keys are pairs of community indices, and values are the DTW distances.\n",
        "    \"\"\"\n",
        "    dtw_distances = {}\n",
        "    for i, j in combinations(range(len(time_sequences)), 2):\n",
        "        seq1 = time_sequences[i]\n",
        "        seq2 = time_sequences[j]\n",
        "\n",
        "        # Calculate the DTW distance\n",
        "        distance, _ = fastdtw(seq1, seq2, dist=euclidean)\n",
        "        dtw_distances[(i, j)] = distance\n",
        "\n",
        "    return dtw_distances"
      ],
      "metadata": {
        "id": "x838L8cYDIIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Combine GCD-11 and DTW Similarities\n",
        "def combine_similarities(gcd_11_distances, dtw_distances):\n",
        "    \"\"\"\n",
        "    Function to combine spatial (GCD-11) and temporal (DTW) similarities.\n",
        "\n",
        "    Args:\n",
        "        gcd_11_distances (dict): GCD-11 distance for each community pair.\n",
        "        dtw_distances (dict): DTW distance for each community pair.\n",
        "    Returns:\n",
        "        combined_distances (dict): Combined distance for each community pair.\n",
        "    \"\"\"\n",
        "    combined_distances = {}\n",
        "    for key in gcd_11_distances:\n",
        "        combined_distances[key] =  gcd_11_distances[key] +  dtw_distances[key]\n",
        "\n",
        "    return combined_distances"
      ],
      "metadata": {
        "id": "M17yAA3FDIKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gcd_11_distances = calculate_gcd_11(community_graphs)\n",
        "dtw_distances = calculate_dtw(community_time_sequences)\n",
        "\n",
        "# Combine the distances\n",
        "combined_distances = combine_similarities(gcd_11_distances, dtw_distances)\n",
        "\n",
        "# Printing the combined similarities for each pair of communities\n",
        "print(\"Combined GCD-11 and DTW Similarities between community pairs:\")\n",
        "for (community_a, community_b), combined_distance in combined_distances.items():\n",
        "    print(f\"Community {community_a} and Community {community_b}: Combined Distance = {combined_distance:.4f}\")"
      ],
      "metadata": {
        "id": "SCfmGdGKDIMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert the combined distances dictionary to a distance matrix\n",
        "def create_distance_matrix(combined_distances, num_communities):\n",
        "    distance_matrix = np.zeros((num_communities, num_communities))\n",
        "    for (i, j), dist in combined_distances.items():\n",
        "        distance_matrix[i, j] = dist\n",
        "        distance_matrix[j, i] = dist  # Since the matrix is symmetric\n",
        "    return distance_matrix\n",
        "\n",
        "# Create the distance matrix\n",
        "num_communities = len(community_graphs)\n",
        "distance_matrix = create_distance_matrix(combined_distances, num_communities)\n",
        "\n",
        "# Ensure it is symmetric and non-negative\n",
        "print(\"Distance Matrix:\")\n",
        "print(distance_matrix)"
      ],
      "metadata": {
        "id": "I6hJ45AJNe73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the Elbow Method to determine the best k\n",
        "distortions = []\n",
        "K = range(2, 6)  # Testing between 2 and 6 clusters\n",
        "for k in K:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(distance_matrix)\n",
        "    distortions.append(kmeans.inertia_)  # Inertia is the within-cluster sum of squares\n",
        "\n",
        "# Plot the Elbow curve\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(K, distortions, 'bx-')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Distortion')\n",
        "plt.title('Elbow Method for Optimal k')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "iM6gTCfiNfBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Elbow Method for Optimal\n",
        "ð‘˜\n",
        "k:\n",
        "\n",
        "This graph plots the distortion (or within-cluster sum of squares) against the number of clusters.\n",
        "The idea is to find the \"elbow point,\" where the rate of distortion reduction significantly slows down. This suggests that increasing the number of clusters beyond that point does not provide much improvement.\n",
        "In this case, the elbow seems to occur around\n",
        "ð‘˜\n",
        "=\n",
        "3\n",
        "k=3, where the distortion starts flattening. This suggests that 3 clusters might be a good choice, as adding more clusters does not reduce distortion significantly."
      ],
      "metadata": {
        "id": "tBagy22aOzRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run K-Means again with the optimal number of clusters\n",
        "optimal_k = 3  # Replace this with the K chosen from Elbow\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
        "final_labels = kmeans.fit_predict(distance_matrix)\n",
        "\n",
        "# Print the final groupings of communities\n",
        "print(\"Final Groupings of Communities:\")\n",
        "for i, label in enumerate(final_labels):\n",
        "    print(f\"Community {i}: Cluster {label}\")"
      ],
      "metadata": {
        "id": "Ou80CMRYNp0q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}